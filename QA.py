# -*- coding: utf-8 -*-
"""T5_for_VQA_generated_captions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xiht660jSbSjETYcgkaaN4jejaTGYQYg
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install sentencepiece --q
!pip install transformers --q
!pip install pytorch-lightning --q

from transformers import T5ForConditionalGeneration, T5Tokenizer
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

MODEL_NAME = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

df = pd.read_csv('/content/drive/MyDrive/ML-VQA/Captions_newly_trained_model.csv')

a = df.iloc[0]
a['question'], a['caption_new'],a['image_id'], a['answers']

import ast
df['answers'] = df['answers'].apply(lambda x: ast.literal_eval(x))

# from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline
# ##Using existing model
# model_name = "MaRiOrOsSi/t5-base-finetuned-question-answering"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelWithLMHead.from_pretrained(model_name)
# question = a['question']
# context = a['caption']
# input = f"question: {question} context: {context}"
# encoded_input = tokenizer([input],
#                              return_tensors='pt',
#                              max_length=512,
#                              truncation=True)
# output = model.generate(input_ids = encoded_input.input_ids,
#                             attention_mask = encoded_input.attention_mask)
# output = tokenizer.decode(output[0], skip_special_tokens=True)
# print(output)

import json
imgs = json.load(open('/content/drive/MyDrive/ML-VQA/captions_val2014.json','r'))

X = {}
for a in imgs['images']:
  X[a['id']] = a

class QADataset(Dataset):
  def __init__(self, data, tokenizer, max_length):
    self.data = data
    self.tokenizer = tokenizer
    self.max_length = max_length

  def __len__(self):
    return len(self.data)

  def __getitem__(self, index: int):
    data_row = self.data.iloc[index]
    question = data_row['question']
    context = data_row['caption_new']
    input = f"question: {question} context: {context}"
    source_encoding = tokenizer(input, max_length = self.max_length, pad_to_max_length = True, padding = "max_length", return_attention_mask = True, add_special_tokens = True, return_tensors = "pt")
    target_encoding = tokenizer(data_row['multiple_choice_answer'], max_length = 30, pad_to_max_length = True, padding = "max_length", return_attention_mask = True, add_special_tokens = True, return_tensors = "pt")

    labels = target_encoding['input_ids']

    return dict(question = data_row["question"], context = data_row['caption'], answer = data_row['multiple_choice_answer'], input_ids = source_encoding['input_ids'].flatten(), attention_mask = source_encoding['attention_mask'].flatten(), labels = labels.flatten())

class QADataModule(pl.LightningDataModule):
  def __init__(self, train_df, val_df, test_df, tokenizer, batch_size, max_length):
    super().__init__()
    self.batch_size = batch_size
    self.train_df = train_df
    self.val_df = val_df
    self.test_df = test_df
    self.tokenizer = tokenizer
    self.max_length = max_length

  def setup(self, stage = None):
    self.train_dataset = QADataset(data = self.train_df,tokenizer=self.tokenizer,max_length=self.max_length)
    self.val_dataset = QADataset(data = self.val_df,tokenizer=self.tokenizer,max_length=self.max_length)
    self.test_dataset = QADataset(data = self.test_df,tokenizer=self.tokenizer,max_length=self.max_length)

  def train_dataloader(self):
    return DataLoader(self.train_dataset,batch_size=self.batch_size, shuffle = True, num_workers=4)

  def val_dataloader(self):
    return DataLoader(self.val_dataset,batch_size=self.batch_size, shuffle = False, num_workers=4)

  def test_dataloader(self):
    return DataLoader(self.test_dataset,batch_size=self.batch_size, shuffle = False, num_workers=4)

BATCH_SIZE = 8
MAX_LEN = 200
train_df, test_df = train_test_split(df, test_size = 0.1)
train_df, val_df = train_test_split(train_df, test_size = 0.1)

from pytorch_lightning.callbacks import ModelCheckpoint
checkpoint_callback = ModelCheckpoint(
    dirpath = 'checkpoints',
    filename = 'best-checkpoint',
    save_top_k = 1,
    verbose = True,
    monitor = "val loss",
    mode = "min"
)

trainer = pl.Trainer(
    callbacks = [checkpoint_callback],
    max_epochs = 3,
    gpus = 1)

tokenizer = T5Tokenizer.from_pretrained('t5-base')

from torch.optim import AdamW
global ip
global am
global l
ip, am, l = None, None, None
class QAModel(pl.LightningModule):
  def __init__(self):
    super().__init__()
    self.model = T5ForConditionalGeneration.from_pretrained('t5-base')

  def forward(self, input_ids, attention_mask, labels = None):
    self.ip = input_ids
    self.a = attention_mask
    self.l = labels
    # print(input_ids, attention_mask, labels)
    # model = T5ForConditionalGeneration.from_pretrained('t5-base')
    output = self.model(input_ids = self.ip,
                            attention_mask = self.a, labels = self.l)
    return output.loss, output.logits

  def training_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self.forward(input_ids, attention_mask, labels)
    self.log("train loss", loss, prog_bar = True, logger = True)
    return loss

  def validation_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self.forward(input_ids, attention_mask, labels)
    self.log("val loss", loss, prog_bar = True, logger = True)
    return loss

  def test_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self.forward(input_ids, attention_mask, labels)
    self.log("test loss", loss, prog_bar = True, logger = True)
    return loss

  def configure_optimizers(self):
    return AdamW(self.parameters(), lr = 0.0001)

model = QAModel()
data_module = QADataModule(train_df, val_df, test_df, tokenizer, 32, MAX_LEN)
trainer.fit(model, data_module)

trainer.test(model, data_module)

!cp checkpoints/best-checkpoint.ckpt /content/drive/MyDrive/ML-VQA

ip = (model.ip)
am = (model.a)
l = (model.l)
model_name = 't5-base'
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)
a = df.iloc[0:10]

# question = a['question']
# context = a['caption']
# input = [f"question: {q} context: {c}" for q,c in zip(question, context)]
# encoded_input = tokenizer(input,
#                              return_tensors='pt',
#                              max_length=500,
#                             pad_to_max_length = True)
# labels = tokenizer(a['multiple_choice_answer'], max_length = 30, pad_to_max_length = True, return_attention_mask = True, add_special_tokens = True, return_tensors = "pt")

# print(am)
output = model(input_ids = ip.to('cpu'),
                            attention_mask = am.to('cpu'), labels = l.to('cpu'))

output.loss

data_module = QADataModule(train_df, val_df, test_df, tokenizer, 32, MAX_LEN)
trained_model = QAModel.load_from_checkpoint('/content/checkpoints/best-checkpoint.ckpt')

trained_model.freeze()

def get_answer(question, context):
  input = f"question: {question} context: {context}"
  source_encoding = tokenizer(input, max_length = MAX_LEN, pad_to_max_length = True, padding = "max_length", return_attention_mask = True, add_special_tokens = True, return_tensors = "pt")
  generated_ids = trained_model.model.generate(
      input_ids = source_encoding['input_ids'],
      attention_mask = source_encoding['attention_mask'],
      num_beams = 1,
      max_length = 80, 
      repetition_penalty = 2.5, 
      early_stopping = True,
      use_cache = True
  )
  preds = [tokenizer.decode(gen_id, skip_special_tokens = True, clean_up_tokenization_spaces = True) for gen_id in generated_ids]
  return " ".join(preds)

idx = 8
test_df.iloc[idx]['question'], test_df.iloc[idx]['caption_new'], test_df.iloc[idx]['multiple_choice_answer']

get_answer(test_df.iloc[idx]['question'], test_df.iloc[idx]['caption_new'])

